{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Layer\n",
    "\n",
    "a common practice is to group conv layer and non-linearity together.\n",
    "\n",
    "conv + relu twice. use variable scope to have reusable code.\n",
    "\n",
    "* Input: [Batch_size, Height, Width, Channels]\n",
    "* Filter: [F_Height, F_Width, Input_Channels, num_filters]\n",
    "* Output: [Batch_size, new_Height, new_Width, num_filters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_relu(inputs, num_filters, k_size, stride, padding, scope_name):\n",
    "    with tf.variable_scope(scope_name, reuse=tf.AUTO_REUSE) as scope:\n",
    "        in_channels = inputs.shape[-1]\n",
    "        kernel = tf.get_variable('kernel', [k_size, k_size, in_channels, num_filters],\n",
    "                                initializer=tf.truncated_normal_initializer())\n",
    "        biases = tf.get_variable('bias', [num_filters], initializer=tf.random_normal_initializer())\n",
    "        \n",
    "        conv = tf.nn.con2d(inputs, kernel, strides=[1,stride,stride,1], padding=padding)\n",
    "        \n",
    "        return tf.nn.relu(conv + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling\n",
    "downsampling technique to reduce the dimensionality of the feature map extracted from the conv layer in order to reduce the processing time.\n",
    "replace a subregion of data with its most representive feature. (max pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxpool(inputs, k_size, stride, padding='VALID', scope_name='pool'):\n",
    "    with tf.variable_scope(scope_name, reuse=tf.AUTO_REUSE) as scope:\n",
    "        pool = tf.nn.max_pool(inputs, ksize=[1, k_size, k_size, 1], strides=[1, stride, stride, 1], padding=padding)\n",
    "    return pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected\n",
    "every neuron in this layer is connected to every neuron in previous layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_connected(inputs, out_dim, scope_name='fc'):\n",
    "    with tf.variable_scope(scope_name, reuse=tf.AUTO.REUSE) as scope:\n",
    "        in_dim = inputs.shape[-1]\n",
    "        w = tf.get_variable('weights', [in_dim, out_dim], initializer=tf.truncted_normal_initializer())\n",
    "        b = tf.get_variable('biases', [out_dim], initializer=tf.constant_initializer(0.0))\n",
    "        output = tf.matmul(inputs, w) + b\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    conv_relu1 = conv_relu(x, num_filters=32, k_size=5, stride=1, padding='SAME', scope_name='conv1')\n",
    "    pool1 = maxpool(conv_relu1, k_size=2, stride=2, padding='VALID', scope_name='pool1')\n",
    "    \n",
    "    conv_relu2 = conv_relu(pool1, num_filters=64, k_size=5, stride=1, padding='SAME', scope_name='conv2')\n",
    "    pool2 = maxpool(conv_relu2, k_size=2, stride=2, padding='VALID', scope_name='pool2')\n",
    "    \n",
    "    feature_dim = pool2.shape[1] * pool2.shape[2] * pool2.shape[3]\n",
    "    pool2_flatten = tf.reshape(pool2, [-1, feature_dim])\n",
    "    \n",
    "    fc = tf.nn.relu(fully_connected(pool2_flatten, 1024, 'fc')) # use 1024 neurons\n",
    "    \n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfkernel",
   "language": "python",
   "name": "tfkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
