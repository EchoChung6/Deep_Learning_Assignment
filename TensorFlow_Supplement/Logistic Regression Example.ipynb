{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logisitic Regression Example\n",
    "\n",
    "* Author: Yingying ZHONG\n",
    "* data: http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Extract MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholder():\n",
    "    \"\"\"\n",
    "    size of mnist data image:  28 * 28 = 784;\n",
    "    0-9 digits recognition hence 10 classes\n",
    "    \n",
    "    Returns:\n",
    "    X -- placeholder for the data input, i.e shape [784, None] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, i.e shape [10, None] and dtype \"float\"\n",
    "    \n",
    "    Notice:\n",
    "    - use None to allow flexibility on the number of examples.\n",
    "      In fact, the number of examples from training set and testing set are different.\n",
    "    \"\"\"\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, [784, None], name=\"weights\") \n",
    "    Y = tf.placeholder(tf.float32, [10, None], name=\"bias\")\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(X, Y):\n",
    "    n = X.shape[0]  # number of input features\n",
    "    C = Y.shape[0]  # number of classes\n",
    "    \n",
    "    W = tf.Variable(np.random.rand(C, n))\n",
    "    b = tf.Variable(tf.zeros(C, 1))\n",
    "    \n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_classifier(X, W, b):\n",
    "    logits = tf.matmul(W, X) + b\n",
    "    pred = tf.nn.softmax(logits)\n",
    "    \n",
    "    return pred # shape [C, m]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notice the difference between \n",
    "\n",
    "* tensorflow tf.nn.softmax \n",
    "\n",
    "* tf.nn.softmax_cross_entropy_with_logits\n",
    "\n",
    "(https://stackoverflow.com/questions/34240703/difference-between-tensorflow-tf-nn-softmax-and-tf-nn-softmax-cross-entropy-with)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The loss function of a single example:\n",
    "$$ L(\\hat{y},  y) =  - \\sum\\limits_{j=1}^C y_j log(\\hat{y_j})$$\n",
    "where C is the number of classes.\n",
    "\n",
    "#### The cost over the entire training set:\n",
    "$$ cost = \\frac{1}{m} \\sum\\limits_{i=1}^m L(\\hat{y^{(i)}}, y^{(i)}) $$\n",
    "where m is the number of data examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(pred, Y):\n",
    "    \"\"\"\n",
    "    \n",
    "    Arguments:\n",
    "    - pred: Y hat that computed using softmax classifier, size of C * m\n",
    "    - Y: the true labels, size of C * m\n",
    "    \n",
    "    notice: \n",
    "    the number of classes C: 10\n",
    "    when calculate the loss, sum over the 10 classes hence along the column\n",
    "    \"\"\"\n",
    "   \n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(pred), reduction_indices = 0))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(learning_rate, cost):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_model(learning_rate = 0.01, num_epochs = 25, \n",
    "                              display_step = 1, minibatch_size = 64):\n",
    "    X, Y = create_placeholder()\n",
    "    W, b = initialize_parameters(X, Y)\n",
    "    pred = softmax_classifier(X, W, b)\n",
    "    cost = compute_cost(pred, Y)\n",
    "    optimizer = gradient_descent(learning_rate, cost)\n",
    "    \n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_cost = 0   # cost relate to an epoch\n",
    "            num_minibatches = int(mnist.train.num_examples/minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            for i in range(batches):\n",
    "                minibatch_x, minibatch_y = mnist.train.next_batch(batch_size)\n",
    "                minibatch_cost, _ = sess.run([cost, optimizer], feed_dict={X: minibatch_x, Y: minibatch_y})\n",
    "                \n",
    "                epoch_cost += minibatch_cost / num_minibatches"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfkernel",
   "language": "python",
   "name": "tfkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
